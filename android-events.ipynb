{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "\n",
    "from moztelemetry import get_pings, get_pings_properties, get_one_ping_per_client\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect some data that can occur in multiple pings per client per day. We'll need to aggregate by client+day, then dump the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "update_channel = \"nightly\"\n",
    "now = dt.datetime.now()\n",
    "start = now - dt.timedelta(3)\n",
    "end = now - dt.timedelta(1)\n",
    "\n",
    "pings = get_pings(sc, app=\"Fennec\", channel=update_channel,\n",
    "                  submission_date=(start.strftime(\"%Y%m%d\"), end.strftime(\"%Y%m%d\")),\n",
    "                  build_id=(\"20100101000000\", \"99999999999999\"),\n",
    "                  fraction=1)\n",
    "\n",
    "subset = get_pings_properties(pings, [\"meta/clientId\",\n",
    "                                      \"meta/documentId\",\n",
    "                                      \"meta/submissionDate\",\n",
    "                                      \"payload/UIMeasurements\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the set of pings, make sure we have actual clientIds and remove duplicate pings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dedupe_pings(rdd):\n",
    "    return rdd.filter(lambda p: p[\"meta/clientId\"] is not None)\\\n",
    "              .map(lambda p: (p[\"meta/clientId\"] + p[\"meta/documentId\"], p))\\\n",
    "              .reduceByKey(lambda x, y: x)\\\n",
    "              .map(lambda x: x[1])\n",
    "\n",
    "subset = dedupe_pings(subset)\n",
    "print subset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to dump each event from the pings. Do a little empty data sanitization so we don't get NoneType errors during the dump. We create a JSON array of active experiments as part of the dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def safe_str(obj):\n",
    "    \"\"\" return the byte string representation of obj \"\"\"\n",
    "    if obj is None:\n",
    "        return unicode(\"\")\n",
    "    return unicode(obj)\n",
    "\n",
    "def transform(ping):    \n",
    "    output = []\n",
    "\n",
    "    clientId = ping[\"meta/clientId\"] # Should not be None since we filter those out\n",
    "    submissionDate = ping[\"meta/submissionDate\"] # Added via the ingestion process so should not be None\n",
    "\n",
    "    events = ping[\"payload/UIMeasurements\"]\n",
    "    if events:\n",
    "        for event in events:\n",
    "            if event[\"type\"] == \"event\":\n",
    "                # Force all fields to strings\n",
    "                timestamp = safe_str(event[\"timestamp\"])\n",
    "                action = safe_str(event[\"action\"])\n",
    "                method = safe_str(event[\"method\"])\n",
    "\n",
    "                # The extras is an optional field\n",
    "                extras = unicode(\"\")\n",
    "                if \"extras\" in event and event[\"extras\"] is not None:\n",
    "                    extras = safe_str(event[\"extras\"])\n",
    "\n",
    "                sessions = {}\n",
    "                experiments = []\n",
    "                for session in event[\"sessions\"]:\n",
    "                    if \"experiment.1:\" in session:\n",
    "                        experiments.append(safe_str(session[13:]))\n",
    "                    elif \"firstrun.1\" in session:\n",
    "                        sessions[unicode(\"firstrun\")] = 1\n",
    "                    elif \"awesomescreen.1\" in session:\n",
    "                        sessions[unicode(\"awesomescreen\")] = 1\n",
    "                    elif \"reader.1\" in session:\n",
    "                        sessions[unicode(\"reader\")] = 1\n",
    "\n",
    "                output.append([clientId, submissionDate, timestamp, action, method, extras, json.dumps(sessions.keys()), json.dumps(experiments)])\n",
    "\n",
    "    return output\n",
    "\n",
    "rawEvents = subset.flatMap(transform)\n",
    "print \"Raw count: \" + str(rawEvents.count())\n",
    "print rawEvents.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can have duplicate events, due to a bug in the data collection that was fixed (bug 1246973). We still need to de-dupe the events. Because pings can be archived on device and submitted on later days, we can't assume dupes only happen on the same submission day. We don't use submission date when de-duping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dedupe_events(rdd):\n",
    "    return rdd.map(lambda p: (p[0] + p[2] + p[3] + p[4], p))\\\n",
    "              .reduceByKey(lambda x, y: x)\\\n",
    "              .map(lambda x: x[1])\n",
    "\n",
    "uniqueEvents = dedupe_events(rawEvents)\n",
    "print \"Unique count: \" + str(uniqueEvents.count())\n",
    "print uniqueEvents.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the set of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = pd.DataFrame(uniqueEvents.collect(), columns=[\"clientid\", \"submissiondate\", \"timestamp\", \"action\", \"method\", \"extras\", \"sessions\", \"experiments\"])\n",
    "!mkdir -p ./output\n",
    "grouped.to_csv(\"./output/fennec-events-\" + update_channel + \"-\" + end.strftime(\"%Y%m%d\") + \".csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "#s3_output = \"s3n://net-mozaws-prod-us-west-2-pipeline-analysis/mfinkle/android_events\"\n",
    "#s3_output += \"/v1/channel=\" + update_channel + \"/end_date=\" + end.strftime(\"%Y%m%d\") \n",
    "#grouped = sqlContext.createDataFrame(transformed, [\"clientid\", \"submissiondate\", \"timestamp\", \"action\", \"method\", \"extras\", \"sessions\", \"experiments\"])\n",
    "#grouped.saveAsParquetFile(s3_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
